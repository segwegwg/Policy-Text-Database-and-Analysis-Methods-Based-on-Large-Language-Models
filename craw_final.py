import requests
from bs4 import BeautifulSoup
import json
import time
import re
import csv
from urllib.parse import urljoin
import os

def load_websites_from_file(filename="websites.txt"):
    """ä»æ–‡ä»¶åŠ è½½ç½‘ç«™åˆ—è¡¨"""
    websites = []
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if line and not line.startswith('#'):
                    websites.append(line)
        print(f"ä» {filename} åŠ è½½äº† {len(websites)} ä¸ªç½‘ç«™")
        return websites
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {filename}")
        return []
    except Exception as e:
        print(f"è¯»å–ç½‘ç«™åˆ—è¡¨æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []

def crawl_multiple_websites():
    """çˆ¬å–å¤šä¸ªç½‘ç«™çš„æ”¿ç­–ä¿¡æ¯"""
    
    # åŠ è½½ç½‘ç«™åˆ—è¡¨
    websites = load_websites_from_file()
    if not websites:
        print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç½‘ç«™åˆ—è¡¨ï¼Œç¨‹åºé€€å‡º")
        return
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    }
    
    all_policy_data = []
    total_policies_crawled = 0
    
    # éå†æ¯ä¸ªç½‘ç«™
    for website_index, list_url in enumerate(websites, 1):
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å–ç¬¬ {website_index}/{len(websites)} ä¸ªç½‘ç«™: {list_url}")
        print(f"{'='*60}")
        
        policy_data = []
        
        try:
            # çˆ¬å–åˆ—è¡¨é¡µ
            response = requests.get(list_url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ”¿ç­–é“¾æ¥ - ä½¿ç”¨æ›´é€šç”¨çš„æ–¹æ³•
            policy_links = extract_policy_links(soup, list_url)
            print(f"ä»è¯¥ç½‘ç«™æ‰¾åˆ° {len(policy_links)} ä¸ªæ”¿ç­–é“¾æ¥")
            
            if not policy_links:
                print("âš ï¸ æœªæ‰¾åˆ°æ”¿ç­–é“¾æ¥ï¼Œè·³è¿‡è¯¥ç½‘ç«™")
                continue
            
            # ç¬¬äºŒçº§ï¼šé€ä¸ªçˆ¬å–æ”¿ç­–è¯¦æƒ…å†…å®¹
            for i, policy in enumerate(policy_links):
                if total_policies_crawled >= 1000:  # å¦‚æœå·²ç»è¾¾åˆ°1000æ¡ï¼Œåœæ­¢çˆ¬å–
                    print("å·²è¾¾åˆ°1000æ¡æ•°æ®ç›®æ ‡ï¼Œåœæ­¢çˆ¬å–")
                    break
                    
                try:
                    print(f"æ­£åœ¨çˆ¬å–ç¬¬ {i+1}/{len(policy_links)} ä¸ªæ”¿ç­–: {policy['title'][:50]}...")
                    
                    # çˆ¬å–è¯¦æƒ…é¡µ
                    detail_response = requests.get(policy['url'], headers=headers, timeout=20)
                    detail_response.encoding = 'utf-8'
                    
                    if detail_response.status_code == 200:
                        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
                        
                        # æå–æ”¿ç­–å†…å®¹
                        content = extract_policy_content(detail_soup)
                        pub_date = extract_publication_date(detail_soup)
                        source = extract_source(detail_soup, list_url)  # æ ¹æ®URLåˆ¤æ–­æ¥æº
                        
                        policy_info = {
                            'title': policy['title'],
                            'url': policy['url'],
                            'publication_date': pub_date,
                            'source': source,
                            'website': list_url,  # è®°å½•æ¥æºç½‘ç«™
                            'content': content,
                            'content_length': len(content),
                            'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S')
                        }
                        
                        policy_data.append(policy_info)
                        total_policies_crawled += 1
                        print(f"âœ“ æˆåŠŸçˆ¬å–å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦ï¼Œç´¯è®¡: {total_policies_crawled} æ¡")
                        
                    else:
                        print(f"âœ— æ— æ³•è®¿é—®é¡µé¢: {detail_response.status_code}")
                    
                    # ç¤¼è²Œå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"âœ— çˆ¬å–å•ä¸ªæ”¿ç­–å¤±è´¥: {e}")
                    continue
            
            # å°†è¯¥ç½‘ç«™çš„æ”¿ç­–æ•°æ®æ·»åŠ åˆ°æ€»æ•°æ®ä¸­
            all_policy_data.extend(policy_data)
            print(f"âœ… å®Œæˆè¯¥ç½‘ç«™çˆ¬å–ï¼Œè·å¾— {len(policy_data)} æ¡æ”¿ç­–")
            
            # ä¿å­˜å½“å‰è¿›åº¦ï¼ˆæ¯ä¸ªç½‘ç«™çˆ¬å–åéƒ½ä¿å­˜ä¸€æ¬¡ï¼‰
            save_progress(all_policy_data, website_index)
            
            if total_policies_crawled >= 1000:
                print("ğŸ¯ å·²è¾¾åˆ°1000æ¡æ•°æ®ç›®æ ‡ï¼")
                break
                
        except Exception as e:
            print(f"âŒ çˆ¬å–ç½‘ç«™ {list_url} æ—¶å‡ºé”™: {e}")
            continue
    
    # æœ€ç»ˆä¿å­˜æ‰€æœ‰æ•°æ®
    if all_policy_data:
        save_final_data(all_policy_data)
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼æ€»å…±çˆ¬å–äº† {len(all_policy_data)} æ¡æ”¿ç­–ä¿¡æ¯")
    else:
        print("æœªæ‰¾åˆ°ä»»ä½•æ”¿ç­–å†…å®¹")

def extract_policy_links(soup, base_url):
    """æå–æ”¿ç­–é“¾æ¥ï¼Œæ”¯æŒå¤šç§ç½‘ç«™ç»“æ„"""
    policy_links = []
    
    # å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
    link_selectors = [
        'a[href*=".shtml"]',
        'a[href*=".html"]',
        'li a',
        '.list a',
        '.news-list a',
        '.content a'
    ]
    
    for selector in link_selectors:
        links = soup.select(selector)
        for link in links:
            try:
                href = link.get('href', '')
                title = link.get_text().strip()
                
                if (href and title and 
                    len(title) > 5 and 
                    any(keyword in title for keyword in ['é€šçŸ¥', 'å…¬å‘Š', 'æŒ‡å—', 'åŠæ³•', 'è§„å®š', 'æ„è§', 'æ–¹æ¡ˆ', 'æ”¿ç­–', 'è§£è¯»'])):
                    
                    # è¡¥å…¨é“¾æ¥
                    full_url = urljoin(base_url, href)
                    
                    policy_links.append({
                        'title': title,
                        'url': full_url
                    })
            except:
                continue
        
        if policy_links:  # å¦‚æœæ‰¾åˆ°é“¾æ¥ï¼Œå°±ä½¿ç”¨è¿™ä¸ªé€‰æ‹©å™¨
            break
    
    return policy_links

def extract_source(soup, website_url):
    """å¢å¼ºç‰ˆæ¥æºæå–å‡½æ•°ï¼šé€šè¿‡URLæ™ºèƒ½è¯†åˆ«å…¨å›½å«å¥å§”ç½‘ç«™æ¥æº"""
    
    # é¦–å…ˆå°è¯•ä»é¡µé¢å…ƒæ•°æ®ä¸­æå–æ¥æº
    meta_source = extract_source_from_meta(soup)
    if meta_source and meta_source != "æœªçŸ¥æ¥æº":
        return clean_source_name(meta_source)
    
    # å…¨å›½å«å¥å§”ç½‘ç«™åŸŸåå…³é”®è¯æ˜ å°„
    region_mapping = {
        # å›½å®¶å±‚é¢
        'nhc.gov.cn': 'å›½å®¶å«å¥å§”',
        
        # ç›´è¾–å¸‚
        'beijing.gov.cn': 'åŒ—äº¬å¸‚å«å¥å§”',
        'sh.gov.cn': 'ä¸Šæµ·å¸‚å«å¥å§”', 
        'tj.gov.cn': 'å¤©æ´¥å¸‚å«å¥å§”',
        'cq.gov.cn': 'é‡åº†å¸‚å«å¥å§”',
        
        # çœ
        'hebei.gov.cn': 'æ²³åŒ—çœå«å¥å§”',
        'shanxi.gov.cn': 'å±±è¥¿çœå«å¥å§”',
        'neimenggu.gov.cn': 'å†…è’™å¤è‡ªæ²»åŒºå«å¥å§”',
        'nmg.gov.cn': 'å†…è’™å¤è‡ªæ²»åŒºå«å¥å§”',
        'liaoning.gov.cn': 'è¾½å®çœå«å¥å§”',
        'ln.gov.cn': 'è¾½å®çœå«å¥å§”',
        'jl.gov.cn': 'å‰æ—çœå«å¥å§”',
        'heilongjiang.gov.cn': 'é»‘é¾™æ±Ÿçœå«å¥å§”',
        'hlj.gov.cn': 'é»‘é¾™æ±Ÿçœå«å¥å§”',
        'jiangsu.gov.cn': 'æ±Ÿè‹çœå«å¥å§”', 
        'js.gov.cn': 'æ±Ÿè‹çœå«å¥å§”',
        'zhejiang.gov.cn': 'æµ™æ±Ÿçœå«å¥å§”',
        'zj.gov.cn': 'æµ™æ±Ÿçœå«å¥å§”',
        'ah.gov.cn': 'å®‰å¾½çœå«å¥å§”',
        'fujian.gov.cn': 'ç¦å»ºçœå«å¥å§”',
        'fj.gov.cn': 'ç¦å»ºçœå«å¥å§”',
        'jiangxi.gov.cn': 'æ±Ÿè¥¿çœå«å¥å§”',
        'jx.gov.cn': 'æ±Ÿè¥¿çœå«å¥å§”',
        'shandong.gov.cn': 'å±±ä¸œçœå«å¥å§”',
        'sd.gov.cn': 'å±±ä¸œçœå«å¥å§”',
        'henan.gov.cn': 'æ²³å—çœå«å¥å§”',
        'ha.gov.cn': 'æ²³å—çœå«å¥å§”',
        'hubei.gov.cn': 'æ¹–åŒ—çœå«å¥å§”',
        'hb.gov.cn': 'æ¹–åŒ—çœå«å¥å§”',
        'hunan.gov.cn': 'æ¹–å—çœå«å¥å§”',
        'hn.gov.cn': 'æ¹–å—çœå«å¥å§”',
        'guangdong.gov.cn': 'å¹¿ä¸œçœå«å¥å§”',
        'gd.gov.cn': 'å¹¿ä¸œçœå«å¥å§”',
        'gx.gov.cn': 'å¹¿è¥¿å£®æ—è‡ªæ²»åŒºå«å¥å§”',
        'hainan.gov.cn': 'æµ·å—çœå«å¥å§”',
        'sc.gov.cn': 'å››å·çœå«å¥å§”',
        'guizhou.gov.cn': 'è´µå·çœå«å¥å§”',
        'gz.gov.cn': 'è´µå·çœå«å¥å§”',
        'yunnan.gov.cn': 'äº‘å—çœå«å¥å§”',
        'yn.gov.cn': 'äº‘å—çœå«å¥å§”',
        'xizang.gov.cn': 'è¥¿è—è‡ªæ²»åŒºå«å¥å§”',
        'xz.gov.cn': 'è¥¿è—è‡ªæ²»åŒºå«å¥å§”',
        'shaanxi.gov.cn': 'é™•è¥¿çœå«å¥å§”',
        'sn.gov.cn': 'é™•è¥¿çœå«å¥å§”',
        'gansu.gov.cn': 'ç”˜è‚ƒçœå«å¥å§”',
        'gs.gov.cn': 'ç”˜è‚ƒçœå«å¥å§”',
        'qinghai.gov.cn': 'é’æµ·çœå«å¥å§”',
        'qh.gov.cn': 'é’æµ·çœå«å¥å§”',
        'ningxia.gov.cn': 'å®å¤å›æ—è‡ªæ²»åŒºå«å¥å§”',
        'nx.gov.cn': 'å®å¤å›æ—è‡ªæ²»åŒºå«å¥å§”',
        'xinjiang.gov.cn': 'æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒºå«å¥å§”',
        'xj.gov.cn': 'æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒºå«å¥å§”',
        
        # ç‰¹åˆ«è¡Œæ”¿åŒº
        'chp.gov.hk': 'é¦™æ¸¯å«ç”Ÿé˜²æŠ¤ä¸­å¿ƒ',
        'health.gov.hk': 'é¦™æ¸¯å«ç”Ÿç½²',
        'ssm.gov.mo': 'æ¾³é—¨å«ç”Ÿå±€',
        'health.gov.mo': 'æ¾³é—¨å«ç”Ÿå±€'
    }
    
    # æ™ºèƒ½è¯†åˆ«æ¥æº
    website_lower = website_url.lower()
    
    # 1. ç²¾ç¡®åŒ¹é…åŸŸåå…³é”®è¯
    for keyword, source_name in region_mapping.items():
        if keyword in website_lower:
            return source_name
    
    # 2. æ¨¡ç³ŠåŒ¹é…ï¼šå°è¯•ä»URLä¸­æå–åœ°åŸŸä¿¡æ¯
    domain_parts = website_lower.split('//')[-1].split('/')[0].split('.')
    if len(domain_parts) >= 2:
        # å¸¸è§çš„äºŒçº§åŸŸåæ¨¡å¼è¯†åˆ«
        region_codes = {
            'bj': 'åŒ—äº¬', 'sh': 'ä¸Šæµ·', 'tj': 'å¤©æ´¥', 'cq': 'é‡åº†',
            'heb': 'æ²³åŒ—', 'sx': 'å±±è¥¿', 'nm': 'å†…è’™å¤', 'ln': 'è¾½å®',
            'jl': 'å‰æ—', 'hlj': 'é»‘é¾™æ±Ÿ', 'js': 'æ±Ÿè‹', 'zj': 'æµ™æ±Ÿ',
            'ah': 'å®‰å¾½', 'fj': 'ç¦å»º', 'jx': 'æ±Ÿè¥¿', 'sd': 'å±±ä¸œ',
            'ha': 'æ²³å—', 'hb': 'æ¹–åŒ—', 'hn': 'æ¹–å—', 'gd': 'å¹¿ä¸œ',
            'gx': 'å¹¿è¥¿', 'hi': 'æµ·å—', 'sc': 'å››å·', 'gz': 'è´µå·',
            'yn': 'äº‘å—', 'xz': 'è¥¿è—', 'sn': 'é™•è¥¿', 'gs': 'ç”˜è‚ƒ',
            'qh': 'é’æµ·', 'nx': 'å®å¤', 'xj': 'æ–°ç–†'
        }
        
        for part in domain_parts:
            if part in region_codes:
                # åˆ¤æ–­æ˜¯çœè¿˜æ˜¯ç›´è¾–å¸‚
                region = region_codes[part]
                if region in ['åŒ—äº¬', 'ä¸Šæµ·', 'å¤©æ´¥', 'é‡åº†']:
                    return f'{region}å¸‚å«å¥å§”'
                else:
                    return f'{region}çœå«å¥å§”'
    
    # 3. æœ€ç»ˆå›é€€æ–¹æ¡ˆ
    return "å›½å®¶å«å¥å§”"

def extract_source_from_meta(soup):
    """ä»é¡µé¢metaæ ‡ç­¾ä¸­å°è¯•æå–æ¥æºä¿¡æ¯"""
    meta_selectors = [
        'meta[name="source"]',
        'meta[name="origin"]',
        'meta[name="publisher"]',
        'meta[property="og:site_name"]'
    ]
    
    for selector in meta_selectors:
        meta_tag = soup.select_one(selector)
        if meta_tag and meta_tag.get('content'):
            content = meta_tag['content'].strip()
            if content and len(content) < 100:
                return content
    return "æœªçŸ¥æ¥æº"

def clean_source_name(source_name):
    """æ¸…æ´—å’Œæ ‡å‡†åŒ–æ¥æºåç§°"""
    if not source_name:
        return "æœªçŸ¥æ¥æº"
    
    # ç§»é™¤å¸¸è§å¹²æ‰°å­—ç¬¦
    clean_name = re.sub(r'[ã€ã€‘\[\]<>ï¼ˆï¼‰()|&nbsp;]', '', source_name.strip())
    
    # æ ‡å‡†åŒ–åç§°
    standardization_map = {
        'å«ç”Ÿå¥åº·å§”å‘˜ä¼š': 'å«å¥å§”',
        'å«ç”Ÿå¥åº·å§”': 'å«å¥å§”',
        'å«ç”Ÿå’Œè®¡åˆ’ç”Ÿè‚²å§”å‘˜ä¼š': 'å«å¥å§”',
        'å«ç”Ÿå±€': 'å«å¥å§”',
        'å«ç”Ÿå…': 'å«å¥å§”',
        'å«ç”Ÿå¥åº·å…': 'å«å¥å§”'
    }
    
    for old, new in standardization_map.items():
        clean_name = clean_name.replace(old, new)
    
    return clean_name

def save_progress(policy_data, website_index):
    """ä¿å­˜çˆ¬å–è¿›åº¦"""
    if policy_data:
        # ä¿å­˜è¿›åº¦æ–‡ä»¶
        progress_file = f'progress_after_website_{website_index}.json'
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(policy_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜åˆ°: {progress_file}")

def save_final_data(policy_data):
    """ä¿å­˜æœ€ç»ˆæ•°æ®"""
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜ä¸ºJSON
    json_file = f'policies_all_websites_{timestamp}.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(policy_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ä¸ºCSVæ‘˜è¦
    csv_file = f'policies_summary_{timestamp}.csv'
    with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['æ ‡é¢˜', 'é“¾æ¥', 'å‘å¸ƒæ—¥æœŸ', 'æ¥æº', 'ç½‘ç«™', 'å†…å®¹é•¿åº¦', 'çˆ¬å–æ—¶é—´'])
        for policy in policy_data:
            writer.writerow([
                policy['title'][:100] + '...' if len(policy['title']) > 100 else policy['title'],
                policy['url'],
                policy['publication_date'],
                policy['source'],
                policy['website'],
                policy['content_length'],
                policy['crawl_time']
            ])
    
    # ä¿å­˜ä¸ºTXT
    txt_file = f'policy_contents_{timestamp}.txt'
    with open(txt_file, 'w', encoding='utf-8') as f:
        for i, policy in enumerate(policy_data, 1):
            f.write(f"ã€ç¬¬{i}æ¡ã€‘{policy['title']}\n")
            f.write(f"ã€é“¾æ¥ã€‘{policy['url']}\n")
            f.write(f"ã€æ—¥æœŸã€‘{policy['publication_date']}\n")
            f.write(f"ã€æ¥æºã€‘{policy['source']}\n")
            f.write(f"ã€ç½‘ç«™ã€‘{policy['website']}\n")
            f.write(f"ã€å†…å®¹ã€‘\n{policy['content']}\n")
            f.write("="*100 + "\n\n")
    
    print(f"ğŸ“Š æœ€ç»ˆæ•°æ®æ–‡ä»¶:")
    print(f"   JSON: {json_file}")
    print(f"   CSV: {csv_file}")
    print(f"   TXT: {txt_file}")

# åŸæœ‰çš„ extract_policy_content, extract_publication_date å‡½æ•°ä¿æŒä¸å˜
def extract_policy_content(soup):
    """æå–æ”¿ç­–æ­£æ–‡å†…å®¹"""
    content_selectors = [
        'div.content',
        'div.TRS_Editor',
        'div.article-content',
        'div.text',
        'div#content',
        'div.main-content',
        '.article-content',
        '.content-main'
    ]
    
    for selector in content_selectors:
        content_div = soup.select_one(selector)
        if content_div:
            for elem in content_div(['script', 'style', 'nav', 'header', 'footer']):
                elem.decompose()
            text = content_div.get_text(separator='\n', strip=True)
            if len(text) > 100:
                return text
    
    body = soup.find('body')
    if body:
        for elem in body(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            elem.decompose()
        return body.get_text(separator='\n', strip=True)
    
    return "æ— æ³•æå–å†…å®¹"

def extract_publication_date(soup):
    """æå–å‘å¸ƒæ—¥æœŸ"""
    date_patterns = [
        r'å‘å¸ƒæ—¶é—´[:ï¼š]\s*(\d{4}-\d{2}-\d{2})',
        r'å‘å¸ƒæ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{2}-\d{2})',
        r'æ—¶é—´[:ï¼š]\s*(\d{4}-\d{2}-\d{2})',
        r'å‘è¡¨æ—¶é—´[:ï¼š]\s*(\d{4}-\d{2}-\d{2})'
    ]
    
    text = soup.get_text()
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)
    
    return "æœªçŸ¥æ—¥æœŸ"

if __name__ == "__main__":
    crawl_multiple_websites()