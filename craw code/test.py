import requests
from bs4 import BeautifulSoup
import json
import time
import re
import csv
from urllib.parse import urljoin

def crawl_nhc_policies_with_content():
    # ç¬¬ä¸€çº§ï¼šçˆ¬å–æ”¿ç­–åˆ—è¡¨
    list_url = "https://www.nhc.gov.cn/wjw/gfxwjj/list.shtml"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    }
    
    policy_data = []
    
    try:
        # çˆ¬å–åˆ—è¡¨é¡µ
        response = requests.get(list_url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ”¿ç­–é“¾æ¥ - æ ¹æ®å®é™…ç½‘é¡µç»“æ„è°ƒæ•´é€‰æ‹©å™¨
        policy_links = []
        
        # æ–¹æ³•1ï¼šå°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ”¿ç­–é“¾æ¥çš„aæ ‡ç­¾
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            # ç­›é€‰å¯èƒ½æ˜¯æ”¿ç­–è¯¦æƒ…é¡µçš„é“¾æ¥
            if ('.shtml' in href and 
                any(keyword in a_tag.get_text() for keyword in ['é€šçŸ¥', 'å…¬å‘Š', 'æŒ‡å—', 'åŠæ³•', 'è§„å®š', 'æ„è§', 'æ–¹æ¡ˆ'])):
                
                # è¡¥å…¨é“¾æ¥
                full_url = urljoin(list_url, href)
                title = a_tag.get_text().strip()
                
                if title and len(title) > 5:  # è¿‡æ»¤æœ‰æ•ˆæ ‡é¢˜
                    policy_links.append({
                        'title': title,
                        'url': full_url
                    })
        
        print(f"æ‰¾åˆ° {len(policy_links)} ä¸ªæ”¿ç­–é“¾æ¥")
        
        # ç¬¬äºŒçº§ï¼šé€ä¸ªçˆ¬å–æ”¿ç­–è¯¦æƒ…å†…å®¹
        for i, policy in enumerate(policy_links[:20]):  # å…ˆæµ‹è¯•å‰20ä¸ªï¼ŒæˆåŠŸåæ”¹ä¸ºlen(policy_links)
            try:
                print(f"æ­£åœ¨çˆ¬å–ç¬¬ {i+1}/{len(policy_links)} ä¸ªæ”¿ç­–: {policy['title']}")
                
                # çˆ¬å–è¯¦æƒ…é¡µ
                detail_response = requests.get(policy['url'], headers=headers, timeout=15)
                detail_response.encoding = 'utf-8'
                
                if detail_response.status_code == 200:
                    detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
                    
                    # æå–æ”¿ç­–å†…å®¹ - è¿™é‡Œéœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´
                    content = extract_policy_content(detail_soup)
                    
                    # æå–å‘å¸ƒæ—¥æœŸå’Œæ¥æº
                    pub_date = extract_publication_date(detail_soup)
                    source = extract_source(detail_soup)
                    
                    policy_info = {
                        'title': policy['title'],
                        'url': policy['url'],
                        'publication_date': pub_date,
                        'source': source,
                        'content': content,
                        'content_length': len(content),
                        'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    policy_data.append(policy_info)
                    print(f"âœ“ æˆåŠŸçˆ¬å–å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
                    
                else:
                    print(f"âœ— æ— æ³•è®¿é—®é¡µé¢: {detail_response.status_code}")
                
                # ç¤¼è²Œå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(2)
                
            except Exception as e:
                print(f"âœ— çˆ¬å–å•ä¸ªæ”¿ç­–å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜å®Œæ•´æ•°æ®
        if policy_data:
            # ä¿å­˜ä¸ºJSON
            with open('policies_with_content.json', 'w', encoding='utf-8') as f:
                json.dump(policy_data, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜ä¸ºCSVï¼ˆä¸å«å†…å®¹ï¼Œå› ä¸ºå†…å®¹å¤ªé•¿ï¼‰
            with open('policies_summary.csv', 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(['æ ‡é¢˜', 'é“¾æ¥', 'å‘å¸ƒæ—¥æœŸ', 'æ¥æº', 'å†…å®¹é•¿åº¦', 'çˆ¬å–æ—¶é—´'])
                for policy in policy_data:
                    writer.writerow([
                        policy['title'], 
                        policy['url'], 
                        policy['publication_date'],
                        policy['source'],
                        policy['content_length'],
                        policy['crawl_time']
                    ])
            
            # å•ç‹¬ä¿å­˜å†…å®¹æ–‡æœ¬æ–‡ä»¶
            with open('policy_contents.txt', 'w', encoding='utf-8') as f:
                for policy in policy_data:
                    f.write(f"ã€æ ‡é¢˜ã€‘{policy['title']}\n")
                    f.write(f"ã€é“¾æ¥ã€‘{policy['url']}\n")
                    f.write(f"ã€æ—¥æœŸã€‘{policy['publication_date']}\n")
                    f.write(f"ã€æ¥æºã€‘{policy['source']}\n")
                    f.write(f"ã€å†…å®¹ã€‘\n{policy['content']}\n")
                    f.write("="*80 + "\n\n")
            
            print(f"ğŸ‰ æˆåŠŸçˆ¬å–å¹¶ä¿å­˜äº† {len(policy_data)} æ¡æ”¿ç­–çš„å®Œæ•´å†…å®¹ï¼")
            print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: policies_with_content.json, policies_summary.csv, policy_contents.txt")
            
        else:
            print("æœªæ‰¾åˆ°ä»»ä½•æ”¿ç­–å†…å®¹")
            
    except Exception as e:
        print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

def extract_policy_content(soup):
    """æå–æ”¿ç­–æ­£æ–‡å†…å®¹"""
    # å°è¯•å¤šç§å¯èƒ½çš„å†…å®¹åŒºåŸŸé€‰æ‹©å™¨
    content_selectors = [
        'div.content',
        'div.TRS_Editor',
        'div.article-content',
        'div.text',
        'div#content',
        'div.main-content'
    ]
    
    for selector in content_selectors:
        content_div = soup.select_one(selector)
        if content_div:
            # æ¸…ç†æ— å…³å…ƒç´ 
            for elem in content_div(['script', 'style', 'nav', 'header', 'footer']):
                elem.decompose()
            
            text = content_div.get_text(separator='\n', strip=True)
            if len(text) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
                return text
    
    # å¦‚æœç‰¹å®šé€‰æ‹©å™¨å¤±è´¥ï¼Œå°è¯•è·å–æ•´ä¸ªæ­£æ–‡çš„æ–‡æœ¬
    body = soup.find('body')
    if body:
        # ç§»é™¤å¯¼èˆªã€é¡µçœ‰é¡µè„šç­‰
        for elem in body(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            elem.decompose()
        
        return body.get_text(separator='\n', strip=True)
    
    return "æ— æ³•æå–å†…å®¹"

def extract_publication_date(soup):
    """æå–å‘å¸ƒæ—¥æœŸ"""
    # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼å’Œä½ç½®
    date_patterns = [
        r'å‘å¸ƒæ—¶é—´[:ï¼š]\s*(\d{4}-\d{2}-\d{2})',
        r'å‘å¸ƒæ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{2}-\d{2})',
        r'æ—¶é—´[:ï¼š]\s*(\d{4}-\d{2}-\d{2})'
    ]
    
    text = soup.get_text()
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)
    
    return "æœªçŸ¥æ—¥æœŸ"

def extract_source(soup):
    """æå–æ¥æºéƒ¨é—¨"""
    source_patterns = [
        r'æ¥æº[:ï¼š]\s*([^\sï¼Œã€‚]+)',
        r'å‘å¸ƒå•ä½[:ï¼š]\s*([^\sï¼Œã€‚]+)'
    ]
    
    text = soup.get_text()
    for pattern in source_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)
    
    return "å›½å®¶å«å¥å§”"

if __name__ == "__main__":
    crawl_nhc_policies_with_content()
