import requests
from bs4 import BeautifulSoup
import json
import time
import re
import csv
from urllib.parse import urljoin
import os

def load_websites_from_file(filename="websites.txt"):
    """ä»æ–‡ä»¶åŠ è½½ç½‘ç«™åˆ—è¡¨"""
    websites = []
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if line and not line.startswith('#'):
                    websites.append(line)
        print(f"ä» {filename} åŠ è½½äº† {len(websites)} ä¸ªç½‘ç«™")
        return websites
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {filename}")
        return []
    except Exception as e:
        print(f"è¯»å–ç½‘ç«™åˆ—è¡¨æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []

def crawl_multiple_websites():
    """çˆ¬å–å¤šä¸ªç½‘ç«™çš„æ”¿ç­–ä¿¡æ¯"""
    
    # åŠ è½½ç½‘ç«™åˆ—è¡¨
    websites = load_websites_from_file()
    if not websites:
        print("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç½‘ç«™åˆ—è¡¨ï¼Œç¨‹åºé€€å‡º")
        return
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    }
    
    all_policy_data = []
    total_policies_crawled = 0
    
    # éå†æ¯ä¸ªç½‘ç«™
    for website_index, list_url in enumerate(websites, 1):
        print(f"\n{'='*60}")
        print(f"å¼€å§‹çˆ¬å–ç¬¬ {website_index}/{len(websites)} ä¸ªç½‘ç«™: {list_url}")
        print(f"{'='*60}")
        
        policy_data = []
        
        try:
            # çˆ¬å–åˆ—è¡¨é¡µ
            response = requests.get(list_url, headers=headers, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ”¿ç­–é“¾æ¥ - ä½¿ç”¨æ›´é€šç”¨çš„æ–¹æ³•
            policy_links = extract_policy_links(soup, list_url)
            print(f"ä»è¯¥ç½‘ç«™æ‰¾åˆ° {len(policy_links)} ä¸ªæ”¿ç­–é“¾æ¥")
            
            if not policy_links:
                print("âš ï¸ æœªæ‰¾åˆ°æ”¿ç­–é“¾æ¥ï¼Œè·³è¿‡è¯¥ç½‘ç«™")
                continue
            
            # ç¬¬äºŒçº§ï¼šé€ä¸ªçˆ¬å–æ”¿ç­–è¯¦æƒ…å†…å®¹
            for i, policy in enumerate(policy_links):
                if total_policies_crawled >= 1000:  # å¦‚æœå·²ç»è¾¾åˆ°1000æ¡ï¼Œåœæ­¢çˆ¬å–
                    print("å·²è¾¾åˆ°1000æ¡æ•°æ®ç›®æ ‡ï¼Œåœæ­¢çˆ¬å–")
                    break
                    
                try:
                    print(f"æ­£åœ¨çˆ¬å–ç¬¬ {i+1}/{len(policy_links)} ä¸ªæ”¿ç­–: {policy['title'][:50]}...")
                    
                    # çˆ¬å–è¯¦æƒ…é¡µ
                    detail_response = requests.get(policy['url'], headers=headers, timeout=20)
                    detail_response.encoding = 'utf-8'
                    
                    if detail_response.status_code == 200:
                        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
                        
                        # æå–æ”¿ç­–å†…å®¹
                        content = extract_policy_content(detail_soup)
                        pub_date = extract_publication_date(detail_soup)
                        source = extract_source(detail_soup, list_url)  # æ ¹æ®URLåˆ¤æ–­æ¥æº
                        
                        policy_info = {
                            'title': policy['title'],
                            'url': policy['url'],
                            'publication_date': pub_date,
                            'source': source,
                            'website': list_url,  # è®°å½•æ¥æºç½‘ç«™
                            'content': content,
                            'content_length': len(content),
                            'crawl_time': time.strftime('%Y-%m-%d %H:%M:%S')
                        }
                        
                        policy_data.append(policy_info)
                        total_policies_crawled += 1
                        print(f"âœ“ æˆåŠŸçˆ¬å–å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦ï¼Œç´¯è®¡: {total_policies_crawled} æ¡")
                        
                    else:
                        print(f"âœ— æ— æ³•è®¿é—®é¡µé¢: {detail_response.status_code}")
                    
                    # ç¤¼è²Œå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"âœ— çˆ¬å–å•ä¸ªæ”¿ç­–å¤±è´¥: {e}")
                    continue
            
            # å°†è¯¥ç½‘ç«™çš„æ”¿ç­–æ•°æ®æ·»åŠ åˆ°æ€»æ•°æ®ä¸­
            all_policy_data.extend(policy_data)
            print(f"âœ… å®Œæˆè¯¥ç½‘ç«™çˆ¬å–ï¼Œè·å¾— {len(policy_data)} æ¡æ”¿ç­–")
            
            # ä¿å­˜å½“å‰è¿›åº¦ï¼ˆæ¯ä¸ªç½‘ç«™çˆ¬å–åéƒ½ä¿å­˜ä¸€æ¬¡ï¼‰
            save_progress(all_policy_data, website_index)
            
            if total_policies_crawled >= 1000:
                print("ğŸ¯ å·²è¾¾åˆ°1000æ¡æ•°æ®ç›®æ ‡ï¼")
                break
                
        except Exception as e:
            print(f"âŒ çˆ¬å–ç½‘ç«™ {list_url} æ—¶å‡ºé”™: {e}")
            continue
    
    # æœ€ç»ˆä¿å­˜æ‰€æœ‰æ•°æ®
    if all_policy_data:
        save_final_data(all_policy_data)
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼æ€»å…±çˆ¬å–äº† {len(all_policy_data)} æ¡æ”¿ç­–ä¿¡æ¯")
    else:
        print("æœªæ‰¾åˆ°ä»»ä½•æ”¿ç­–å†…å®¹")

def extract_policy_links(soup, base_url):
    """æå–æ”¿ç­–é“¾æ¥ï¼Œæ”¯æŒå¤šç§ç½‘ç«™ç»“æ„"""
    policy_links = []
    
    # å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
    link_selectors = [
        'a[href*=".shtml"]',
        'a[href*=".html"]',
        'li a',
        '.list a',
        '.news-list a',
        '.content a'
    ]
    
    for selector in link_selectors:
        links = soup.select(selector)
        for link in links:
            try:
                href = link.get('href', '')
                title = link.get_text().strip()
                
                if (href and title and 
                    len(title) > 5 and 
                    any(keyword in title for keyword in ['é€šçŸ¥', 'å…¬å‘Š', 'æŒ‡å—', 'åŠæ³•', 'è§„å®š', 'æ„è§', 'æ–¹æ¡ˆ', 'æ”¿ç­–', 'è§£è¯»'])):
                    
                    # è¡¥å…¨é“¾æ¥
                    full_url = urljoin(base_url, href)
                    
                    policy_links.append({
                        'title': title,
                        'url': full_url
                    })
            except:
                continue
        
        if policy_links:  # å¦‚æœæ‰¾åˆ°é“¾æ¥ï¼Œå°±ä½¿ç”¨è¿™ä¸ªé€‰æ‹©å™¨
            break
    
    return policy_links

def extract_source(soup, website_url):
    """æå–æ¥æºéƒ¨é—¨ï¼Œæ ¹æ®ç½‘ç«™URLåˆ¤æ–­é»˜è®¤æ¥æº"""
    source_patterns = [
        r'æ¥æº[:ï¼š]\s*([^\sï¼Œã€‚]+)',
        r'å‘å¸ƒå•ä½[:ï¼š]\s*([^\sï¼Œã€‚]+)',
        r'æ¥æº[:ï¼š]\s*([^<]+)'
    ]
    
    text = soup.get_text()
    for pattern in source_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
    
    # æ ¹æ®URLåˆ¤æ–­é»˜è®¤æ¥æº
    if 'beijing' in website_url:
        return "åŒ—äº¬å¸‚å«å¥å§”"
    elif 'sh.gov' in website_url:
        return "ä¸Šæµ·å¸‚å«å¥å§”"
    elif 'gd.gov' in website_url:
        return "å¹¿ä¸œçœå«å¥å§”"
    elif 'zj.gov' in website_url:
        return "æµ™æ±Ÿçœå«å¥å§”"
    elif 'jiangsu' in website_url:
        return "æ±Ÿè‹çœå«å¥å§”"
    elif 'sc.gov' in website_url:
        return "å››å·çœå«å¥å§”"
    elif 'hubei' in website_url:
        return "æ¹–åŒ—çœå«å¥å§”"
    elif 'hunan' in website_url:
        return "æ¹–å—çœå«å¥å§”"
    elif 'henan' in website_url:
        return "æ²³å—çœå«å¥å§”"
    else:
        return "å›½å®¶å«å¥å§”"

def save_progress(policy_data, website_index):
    """ä¿å­˜çˆ¬å–è¿›åº¦"""
    if policy_data:
        # ä¿å­˜è¿›åº¦æ–‡ä»¶
        progress_file = f'progress_after_website_{website_index}.json'
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(policy_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜åˆ°: {progress_file}")

def save_final_data(policy_data):
    """ä¿å­˜æœ€ç»ˆæ•°æ®"""
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜ä¸ºJSON
    json_file = f'policies_all_websites_{timestamp}.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(policy_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ä¸ºCSVæ‘˜è¦
    csv_file = f'policies_summary_{timestamp}.csv'
    with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['æ ‡é¢˜', 'é“¾æ¥', 'å‘å¸ƒæ—¥æœŸ', 'æ¥æº', 'ç½‘ç«™', 'å†…å®¹é•¿åº¦', 'çˆ¬å–æ—¶é—´'])
        for policy in policy_data:
            writer.writerow([
                policy['title'][:100] + '...' if len(policy['title']) > 100 else policy['title'],
                policy['url'],
                policy['publication_date'],
                policy['source'],
                policy['website'],
                policy['content_length'],
                policy['crawl_time']
            ])
    
    # ä¿å­˜ä¸ºTXT
    txt_file = f'policy_contents_{timestamp}.txt'
    with open(txt_file, 'w', encoding='utf-8') as f:
        for i, policy in enumerate(policy_data, 1):
            f.write(f"ã€ç¬¬{i}æ¡ã€‘{policy['title']}\n")
            f.write(f"ã€é“¾æ¥ã€‘{policy['url']}\n")
            f.write(f"ã€æ—¥æœŸã€‘{policy['publication_date']}\n")
            f.write(f"ã€æ¥æºã€‘{policy['source']}\n")
            f.write(f"ã€ç½‘ç«™ã€‘{policy['website']}\n")
            f.write(f"ã€å†…å®¹ã€‘\n{policy['content']}\n")
            f.write("="*100 + "\n\n")
    
    print(f"ğŸ“Š æœ€ç»ˆæ•°æ®æ–‡ä»¶:")
    print(f"   JSON: {json_file}")
    print(f"   CSV: {csv_file}")
    print(f"   TXT: {txt_file}")

# åŸæœ‰çš„ extract_policy_content, extract_publication_date å‡½æ•°ä¿æŒä¸å˜
def extract_policy_content(soup):
    """æå–æ”¿ç­–æ­£æ–‡å†…å®¹"""
    content_selectors = [
        'div.content',
        'div.TRS_Editor',
        'div.article-content',
        'div.text',
        'div#content',
        'div.main-content',
        '.article-content',
        '.content-main'
    ]
    
    for selector in content_selectors:
        content_div = soup.select_one(selector)
        if content_div:
            for elem in content_div(['script', 'style', 'nav', 'header', 'footer']):
                elem.decompose()
            text = content_div.get_text(separator='\n', strip=True)
            if len(text) > 100:
                return text
    
    body = soup.find('body')
    if body:
        for elem in body(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            elem.decompose()
        return body.get_text(separator='\n', strip=True)
    
    return "æ— æ³•æå–å†…å®¹"

def extract_publication_date(soup):
    """æå–å‘å¸ƒæ—¥æœŸ"""
    date_patterns = [
        r'å‘å¸ƒæ—¶é—´[:ï¼š]\s*(\d{4}-\d{2}-\d{2})',
        r'å‘å¸ƒæ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{2}-\d{2})',
        r'æ—¶é—´[:ï¼š]\s*(\d{4}-\d{2}-\d{2})',
        r'å‘è¡¨æ—¶é—´[:ï¼š]\s*(\d{4}-\d{2}-\d{2})'
    ]
    
    text = soup.get_text()
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)
    
    return "æœªçŸ¥æ—¥æœŸ"

if __name__ == "__main__":
    crawl_multiple_websites()